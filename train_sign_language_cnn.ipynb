{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c479cf",
   "metadata": {},
   "source": [
    "# Entraînement du modèle CNN pour la reconnaissance du langage des signes\n",
    "\n",
    "Ce notebook entraîne un modèle CNN capable de reconnaître les lettres et chiffres du langage des signes.\n",
    "Le modèle sera sauvegardé dans `model/sign_language_cnn.h5` pour être utilisé par l'application Flask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa078ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation de TensorFlow (cela peut prendre quelques minutes)...\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1. INSTALLATION & IMPORTS (robuste)\n",
    "# =========================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def ensure_package(pkg_spec):\n",
    "    \"\"\"Install a package if it's not importable.\"\"\"\n",
    "    name = pkg_spec.split('==')[0].split('>=')[0].split('<=')[0]\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "    except Exception:\n",
    "        print(f'Installation de {pkg_spec}...')\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg_spec])\n",
    "\n",
    "# Ensure typing_extensions recent enough for IPython/guarded_eval (avoid kernel crashes)\\\n",
    "# we pin a minimum version but allow pip to choose a compatible one.\n",
    "ensure_package('typing_extensions>=4.5.0')\n",
    "\n",
    "# TensorFlow et kagglehub : n'installer que si nécessaire pour éviter des réinstallations fréquentes.\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception:\n",
    "    print('TensorFlow non trouvé : installation en cours (cela peut prendre plusieurs minutes)...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.13.0'])\n",
    "    import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import kagglehub\n",
    "except Exception:\n",
    "    print('kagglehub non trouvé : installation en cours...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'kagglehub'])\n",
    "    import kagglehub\n",
    "\n",
    "# Imports principaux\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "# Keras peut provenir de tensorflow.keras\n",
    "try:\n",
    "    print(f'Keras version: {keras.__version__}')\n",
    "except Exception:\n",
    "    print('Keras: version indisponible via keras.__version__')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bdb44",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'environnement et paramètres\n",
    "\n",
    "Définissez les chemins et hyperparamètres pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924489b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "# Chemins\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "MODEL_PATH = MODEL_DIR / \"sign_language_cnn.h5\"\n",
    "\n",
    "# Créer le dossier model s'il n'existe pas (parents=True pour robustesse)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparamètres\n",
    "IMG_SIZE = (64, 64)      # Taille des images (64x64 comme dans ton config.py)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "SEED = 123\n",
    "\n",
    "print(f\"Dossier du modèle: {MODEL_DIR}\")\n",
    "print(f\"Chemin du modèle final: {MODEL_PATH}\")\n",
    "print(f\"Taille des images: {IMG_SIZE}\")\n",
    "print(f\"Nombre d'époques: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d73e9",
   "metadata": {},
   "source": [
    "## 3. Télécharger le dataset depuis Kaggle\n",
    "\n",
    "Utilise `kagglehub` pour télécharger le dataset Sign Language de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. TÉLÉCHARGER LE DATASET\n",
    "# =========================\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"Téléchargement du dataset Sign Language depuis Kaggle...\")\n",
    "print(\"(Cela peut prendre plusieurs minutes la première fois)\")\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"harshvardhan21/sign-language-detection-using-images\")\n",
    "dataset_path = Path(dataset_path)\n",
    "print(f\"Dataset téléchargé dans: {dataset_path}\")\n",
    "\n",
    "# Repérer le dossier contenant les images de classes (qui contient des sous-dossiers par classe)\n",
    "candidates = [dataset_path / 'data', dataset_path]\n",
    "data_dir = None\n",
    "for cand in candidates:\n",
    "    if cand.exists() and cand.is_dir():\n",
    "        subdirs = [d for d in cand.iterdir() if d.is_dir()]\n",
    "        if subdirs and any(any(d.glob('*.jpg')) or any(d.glob('*.png')) for d in subdirs):\n",
    "            data_dir = cand\n",
    "            break\n",
    "\n",
    "# Si non trouvé, chercher plus profondément\n",
    "if data_dir is None:\n",
    "    for p in dataset_path.rglob('*'):\n",
    "        if p.is_dir():\n",
    "            subdirs = [d for d in p.iterdir() if d.is_dir()]\n",
    "            if len(subdirs) >= 2 and any(any(d.glob('*.jpg')) or any(d.glob('*.png')) for d in subdirs):\n",
    "                data_dir = p\n",
    "                break\n",
    "\n",
    "if data_dir is None:\n",
    "    raise FileNotFoundError('Impossible de localiser le dossier dimages dans le dataset téléchargé. Vérifiez dataset_path.')\n",
    "\n",
    "print(f\"Dossier des images trouvé: {data_dir}\")\n",
    "\n",
    "# Lister les classes (sous-dossiers)\n",
    "classes = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
    "print(f\"Classes trouvées ({len(classes)}): {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad039c00",
   "metadata": {},
   "source": [
    "## 4. Créer les datasets d'entraînement et de validation\n",
    "\n",
    "Charger les images et les séparer en ensembles d'entraînement (80%) et de validation (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5520fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. CRÉER LES DATASETS\n",
    "# =========================\n",
    "\n",
    "print(\"Chargement du dataset d'entraînement...\")\n",
    "try:\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        str(data_dir),\n",
    "        validation_split=0.2,\n",
    "        subset=\"training\",\n",
    "        seed=SEED,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        str(data_dir),\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\",\n",
    "        seed=SEED,\n",
    "        image_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Erreur lors du chargement des images depuis {data_dir}: {e}')\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes trouvées: {class_names}\")\n",
    "print(f\"Nombre de classes: {num_classes}\")\n",
    "\n",
    "# Optimiser le pipeline\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f1bce",
   "metadata": {},
   "source": [
    "## 5. Augmentation des données\n",
    "\n",
    "Appliquer des transformations aléatoires (flip, rotation, zoom) pour améliorer la robustesse du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f554d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. AUGMENTATION DE DONNÉES\n",
    "# =========================\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "print(\"Pipeline d'augmentation de données créé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d0691",
   "metadata": {},
   "source": [
    "## 6. Construire le modèle CNN\n",
    "\n",
    "Créer un modèle avec 3 blocs Conv2D + MaxPooling, suivi de couches denses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. CONSTRUIRE LE MODÈLE CNN\n",
    "# =========================\n",
    "\n",
    "print(\"Construction du modèle CNN...\")\n",
    "\n",
    "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "# Bloc 1\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Bloc 2\n",
    "x = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Bloc 3\n",
    "x = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Couches denses\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "print(\"Résumé du modèle:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749642",
   "metadata": {},
   "source": [
    "## 7. Compiler et entraîner le modèle\n",
    "\n",
    "Compiler le modèle avec Adam et sparse_categorical_crossentropy, puis l'entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f40987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. COMPILER ET ENTRAÎNER\n",
    "# =========================\n",
    "\n",
    "print(\"Compilation du modèle...\")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nEntraînement en cours ({EPOCHS} époques)...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Entraînement terminé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658747ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 8. SAUVEGARDER LE MODÈLE\n",
    "# =========================\n",
    "\n",
    "print(\"\\nSauvegarde du modèle...\")\n",
    "\n",
    "# S'assurer que le dossier model existe\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save(str(MODEL_PATH))\n",
    "\n",
    "print(f\"✅ Modèle sauvegardé dans: {MODEL_PATH}\")\n",
    "print(f\"Taille du fichier: {MODEL_PATH.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Vérifier que le fichier existe\n",
    "if MODEL_PATH.exists():\n",
    "    print(\"✅ Fichier vérifié - Le modèle est prêt pour Flask!\")\n",
    "else:\n",
    "    print(\"❌ Erreur: Le fichier n'a pas été créé\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
