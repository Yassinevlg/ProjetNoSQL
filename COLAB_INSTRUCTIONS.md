# Guide Complet: EntraÃ®ner le ModÃ¨le CNN sur Google Colab

## ProblÃ¨me
TensorFlow n'est pas disponible pour Python 3.14 sur Windows. Conda n'est pas installÃ© sur ta machine.

## Solution
Utiliser Google Colab (gratuit, GPU inclus, pas d'installation locale) pour entraÃ®ner le modÃ¨le, puis tÃ©lÃ©charger le fichier `sign_language_cnn.h5` dans ta machine locale.

## Ã‰tapes

### 1. Ouvrir Google Colab
- Va Ã  https://colab.research.google.com
- Clique sur "New Notebook" (ou File > New Notebook)
- Tu vas Ãªtre invitÃ© Ã  te connecter avec un compte Google (gratuit)

### 2. Copier et exÃ©cuter les cellules de code ci-dessous

**Cellule 1: Installations et imports**
```python
# Installations
!pip install kagglehub tensorflow==2.13.0 -q

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # RÃ©duire le bruit des logs TF

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import kagglehub
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
```

**Cellule 2: Configuration et tÃ©lÃ©chargement du dataset**
```python
# Configuration
IMG_SIZE = 64
BATCH_SIZE = 32
EPOCHS = 15

# CrÃ©er un dossier temporaire pour les donnÃ©es
data_dir = Path('/tmp/sign_language_data')
data_dir.mkdir(exist_ok=True, parents=True)

# TÃ©lÃ©charger le dataset depuis Kaggle
# Note: Tu dois avoir tes credentials Kaggle disponibles
# Si tu n'as pas d'API key Kaggle, va Ã :
# https://www.kaggle.com/settings/account -> Create New API Token
# Colab va te demander d'uploader le fichier kaggle.json

try:
    dataset_path = kagglehub.dataset_download("grassknoted/asl-alphabet")
    print(f"Dataset tÃ©lÃ©chargÃ© vers: {dataset_path}")
except Exception as e:
    print(f"Erreur lors du tÃ©lÃ©chargement: {e}")
    print("Assure-toi que ton API key Kaggle est configurÃ©e")
    print("File -> Upload file -> kaggle.json")
```

**Cellule 3: CrÃ©er les datasets train/val avec augmentation**
```python
# Augmentation de donnÃ©es
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
])

# CrÃ©er les datasets
train_dataset = keras.utils.image_dataset_from_directory(
    dataset_path,
    seed=42,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="training"
)

val_dataset = keras.utils.image_dataset_from_directory(
    dataset_path,
    seed=42,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="validation"
)

print(f"Nombre de classes: {len(train_dataset.class_names)}")
print(f"Noms des classes: {train_dataset.class_names}")

# Normaliser les images (0-1)
normalization_layer = layers.Rescaling(1./255)
train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))
val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))

# Appliquer l'augmentation au dataset d'entraÃ®nement
train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x), y))
```

**Cellule 4: Construire l'architecture CNN**
```python
model = models.Sequential([
    # Bloc 1
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Bloc 2
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Bloc 3
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Couches denses
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(len(train_dataset.class_names), activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
```

**Cellule 5: EntraÃ®ner le modÃ¨le (â±ï¸ ~15-30 min)**
```python
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS,
    verbose=1
)
```

**Cellule 6: Visualiser les rÃ©sultats**
```python
# Tracer l'accuracy et la loss
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

axes[0].plot(history.history['accuracy'], label='Train Accuracy')
axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')
axes[0].set_title('Accuracy au fil des epochs')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()
axes[0].grid(True)

axes[1].plot(history.history['loss'], label='Train Loss')
axes[1].plot(history.history['val_loss'], label='Val Loss')
axes[1].set_title('Loss au fil des epochs')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

print(f"\nAccuracy finale sur validation: {history.history['val_accuracy'][-1]:.2%}")
```

**Cellule 7: Sauvegarder et tÃ©lÃ©charger le modÃ¨le**
```python
# Sauvegarder le modÃ¨le en format .h5
model.save('/content/sign_language_cnn.h5')
print("âœ“ ModÃ¨le sauvegardÃ© Ã  /content/sign_language_cnn.h5")

# Colab va te permettre de tÃ©lÃ©charger le fichier automatiquement
# Le fichier apparaÃ®tra dans le panneau de fichiers Ã  gauche
```

## 3. TÃ©lÃ©charger le fichier sur ta machine

AprÃ¨s l'exÃ©cution de la **Cellule 7**:
1. Dans le panneau de fichiers Colab (Ã  gauche), tu verras `sign_language_cnn.h5`
2. Clique sur les 3 points `â‹¯` Ã  cÃ´tÃ© du fichier
3. SÃ©lectionne "Download"
4. Le fichier va Ãªtre tÃ©lÃ©chargÃ© dans `C:\Users\yassi\Downloads\sign_language_cnn.h5`

## 4. Placer le fichier dans ton projet

```powershell
# Ouvre un terminal PowerShell et exÃ©cute:
Move-Item -Path "C:\Users\yassi\Downloads\sign_language_cnn.h5" `
          -Destination "C:\Users\yassi\.gemini\antigravity\scratch\cnn-mongodb-project\model\sign_language_cnn.h5" `
          -Force
```

Ou manuellement:
1. Navigue Ã  `C:\Users\yassi\Downloads\`
2. Copie `sign_language_cnn.h5`
3. Ouvre `C:\Users\yassi\.gemini\antigravity\scratch\cnn-mongodb-project\model\`
4. Colle le fichier

## 5. Relancer ton application Flask

```powershell
# Dans le terminal oÃ¹ tu lances ton app:
cd C:\Users\yassi\.gemini\antigravity\scratch\cnn-mongodb-project
python app.py
```

L'app va automatiquement charger le modÃ¨le. Les prÃ©dictions ne seront plus alÃ©atoires! ğŸ‰

## DÃ©pannage

**Q: Je dois obtenir une API key Kaggle?**
R: Oui, pour tÃ©lÃ©charger le dataset. Va Ã  https://www.kaggle.com/settings/account et crÃ©e une API token. Colab te permettra d'uploader le fichier `kaggle.json`.

**Q: Ã‡a prend combien de temps?**
R: ~15-30 minutes avec le GPU gratuit de Colab (beaucoup plus rapide que sur ta machine locale).

**Q: Je peux utiliser CPU au lieu du GPU?**
R: Oui, mais ce sera plus lent (~1-2 heures). GPU est recommandÃ© (gratuit dans Colab).

**Q: Que faire si j'ai une erreur?**
R: ExÃ©cute les cellules une par une et lis les messages d'erreur attentivement. 90% des problÃ¨mes viennent de:
- API key Kaggle manquante â†’ Va uploader kaggle.json
- MÃ©moire insuffisante â†’ RÃ©duis BATCH_SIZE Ã  16
- GPU non activÃ© â†’ Clique sur Runtime > Change runtime type > GPU

Bon entraÃ®nement! ğŸš€
