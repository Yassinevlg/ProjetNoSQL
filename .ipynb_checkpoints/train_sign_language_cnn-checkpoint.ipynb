{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c479cf",
   "metadata": {},
   "source": [
    "# Entraînement du modèle CNN pour la reconnaissance du langage des signes\n",
    "\n",
    "Ce notebook entraîne un modèle CNN capable de reconnaître les lettres et chiffres du langage des signes.\n",
    "Le modèle sera sauvegardé dans `model/sign_language_cnn.h5` pour être utilisé par l'application Flask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa078ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. INSTALLATION & IMPORTS\n",
    "# =========================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Installer tensorflow/keras si nécessaire\n",
    "print(\"Installation de TensorFlow (cela peut prendre quelques minutes)...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"tensorflow>=2.13.0\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"kagglehub\"])\n",
    "\n",
    "# Imports principaux\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bdb44",
   "metadata": {},
   "source": [
    "## 2. Configuration de l'environnement et paramètres\n",
    "\n",
    "Définissez les chemins et hyperparamètres pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924489b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "# Chemins\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "MODEL_PATH = MODEL_DIR / \"sign_language_cnn.h5\"\n",
    "\n",
    "# Créer le dossier model s'il n'existe pas\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Hyperparamètres\n",
    "IMG_SIZE = (64, 64)      # Taille des images (64x64 comme dans ton config.py)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "SEED = 123\n",
    "\n",
    "print(f\"Dossier du modèle: {MODEL_DIR}\")\n",
    "print(f\"Chemin du modèle final: {MODEL_PATH}\")\n",
    "print(f\"Taille des images: {IMG_SIZE}\")\n",
    "print(f\"Nombre d'époques: {EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d73e9",
   "metadata": {},
   "source": [
    "## 3. Télécharger le dataset depuis Kaggle\n",
    "\n",
    "Utilise `kagglehub` pour télécharger le dataset Sign Language de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. TÉLÉCHARGER LE DATASET\n",
    "# =========================\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "print(\"Téléchargement du dataset Sign Language depuis Kaggle...\")\n",
    "print(\"(Cela peut prendre plusieurs minutes la première fois)\")\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"harshvardhan21/sign-language-detection-using-images\")\n",
    "print(f\"Dataset téléchargé dans: {dataset_path}\")\n",
    "\n",
    "# Trouver le dossier contenant les images (structure typique: data/)\n",
    "data_dir = Path(dataset_path) / \"data\"\n",
    "if not data_dir.exists():\n",
    "    # Fallback: chercher un dossier qui contient des sous-dossiers avec des images\n",
    "    for p in Path(dataset_path).rglob(\"*\"):\n",
    "        if p.is_dir() and any(p.glob(\"*.jpg\")) or any(p.glob(\"*.png\")):\n",
    "            data_dir = p.parent\n",
    "            break\n",
    "\n",
    "print(f\"Dossier des images trouvé: {data_dir}\")\n",
    "\n",
    "# Lister les classes (sous-dossiers)\n",
    "classes = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
    "print(f\"Classes trouvées ({len(classes)}): {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad039c00",
   "metadata": {},
   "source": [
    "## 4. Créer les datasets d'entraînement et de validation\n",
    "\n",
    "Charger les images et les séparer en ensembles d'entraînement (80%) et de validation (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5520fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4. CRÉER LES DATASETS\n",
    "# =========================\n",
    "\n",
    "print(\"Chargement du dataset d'entraînement...\")\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"Chargement du dataset de validation...\")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes trouvées: {class_names}\")\n",
    "print(f\"Nombre de classes: {num_classes}\")\n",
    "\n",
    "# Optimiser le pipeline\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f1bce",
   "metadata": {},
   "source": [
    "## 5. Augmentation des données\n",
    "\n",
    "Appliquer des transformations aléatoires (flip, rotation, zoom) pour améliorer la robustesse du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f554d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5. AUGMENTATION DE DONNÉES\n",
    "# =========================\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "print(\"Pipeline d'augmentation de données créé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d0691",
   "metadata": {},
   "source": [
    "## 6. Construire le modèle CNN\n",
    "\n",
    "Créer un modèle avec 3 blocs Conv2D + MaxPooling, suivi de couches denses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a70ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6. CONSTRUIRE LE MODÈLE CNN\n",
    "# =========================\n",
    "\n",
    "print(\"Construction du modèle CNN...\")\n",
    "\n",
    "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "# Bloc 1\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Bloc 2\n",
    "x = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Bloc 3\n",
    "x = layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "# Couches denses\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "print(\"Résumé du modèle:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe749642",
   "metadata": {},
   "source": [
    "## 7. Compiler et entraîner le modèle\n",
    "\n",
    "Compiler le modèle avec Adam et sparse_categorical_crossentropy, puis l'entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f40987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 7. COMPILER ET ENTRAÎNER\n",
    "# =========================\n",
    "\n",
    "print(\"Compilation du modèle...\")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nEntraînement en cours ({EPOCHS} époques)...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Entraînement terminé!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
