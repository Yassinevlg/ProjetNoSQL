{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f46ab9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q tensorflow==2.20.0 kagglehub matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e461c43",
      "metadata": {},
      "source": [
        "## Kaggle credentials\n",
        "If you haven't already configured Kaggle in Colab, upload your `kaggle.json` (from https://www.kaggle.com/me/account). The cell below opens a file picker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b9d483",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "# Upload kaggle.json if needed\n",
        "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    print('Upload your kaggle.json (you will be prompted) or skip if already configured')\n",
        "    uploaded = files.upload()\n",
        "    if 'kaggle.json' in uploaded:\n",
        "        os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "        with open('/root/.kaggle/kaggle.json','wb') as f:\n",
        "            f.write(uploaded['kaggle.json'])\n",
        "        os.chmod('/root/.kaggle/kaggle.json', 0o600)\n",
        "        print('kaggle.json saved.')\n",
        "    else:\n",
        "        print('No kaggle.json uploaded. If dataset requires Kaggle auth, provide kaggle.json and re-run.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc97bf62",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset via kagglehub (uses Kaggle API)\n",
        "import kagglehub, os\n",
        "print('Starting dataset download...')\n",
        "dataset_path = kagglehub.dataset_download('harshvardhan21/sign-language-detection-using-images')\n",
        "dataset_path = os.path.abspath(dataset_path)\n",
        "print('Downloaded to', dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d203f5fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Locate image folder inside the downloaded dataset\n",
        "from pathlib import Path\n",
        "p = Path(dataset_path)\n",
        "candidates = [p / 'data', p]\n",
        "data_dir = None\n",
        "for cand in candidates:\n",
        "    if cand.exists() and cand.is_dir():\n",
        "        subdirs = [d for d in cand.iterdir() if d.is_dir()]\n",
        "        if subdirs and any(any(d.glob('*.jpg')) or any(d.glob('*.png')) for d in subdirs):\n",
        "            data_dir = cand\n",
        "            break\n",
        "if data_dir is None:\n",
        "    for p2 in p.rglob('*'):\n",
        "        if p2.is_dir():\n",
        "            subdirs = [d for d in p2.iterdir() if d.is_dir()]\n",
        "            if len(subdirs) >= 2 and any(any(d.glob('*.jpg')) or any(d.glob('*.png')) for d in subdirs):\n",
        "                data_dir = p2\n",
        "                break\n",
        "if data_dir is None:\n",
        "    raise FileNotFoundError('Could not locate images directory in the downloaded dataset')\n",
        "print('Images directory:', data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df88e78e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN/VAL dataset creation using TensorFlow utilities\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (64,64)\n",
        "BATCH_SIZE = 32\n",
        "SEED = 123\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(str(data_dir), validation_split=0.2, subset='training', seed=SEED, image_size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(str(data_dir), validation_split=0.2, subset='validation', seed=SEED, image_size=IMG_SIZE, batch_size=BATCH_SIZE)\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print('Classes:', class_names)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f881d20e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definition\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(*IMG_SIZE,3))\n",
        "x = layers.RandomFlip('horizontal')(inputs)\n",
        "x = layers.RandomRotation(0.1)(x)\n",
        "x = layers.RandomZoom(0.1)(x)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(32,3,activation='relu',padding='same')(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Conv2D(64,3,activation='relu',padding='same')(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Conv2D(128,3,activation='relu',padding='same')(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(256,activation='relu')(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d35a74",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile & train\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "EPOCHS = 15\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20adc5f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model under two filenames so the Flask app can find it\n",
        "model_dir = '/content/model'\n",
        "import os\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "# Primary name used by the notebook\n",
        "model_path = os.path.join(model_dir, 'sign_language_cnn.h5')\n",
        "model.save(model_path)\n",
        "# Also save with the name expected by the Flask app (cnn_model.h5)\n",
        "cnn_model_path = os.path.join(model_dir, 'cnn_model.h5')\n",
        "model.save(cnn_model_path)\n",
        "print('Saved model to', model_path)\n",
        "print('Also saved model to', cnn_model_path)\n",
        "# Provide download links in Colab for both files\n",
        "from google.colab import files\n",
        "files.download(model_path)\n",
        "files.download(cnn_model_path)\n",
        "# If you want to directly copy to a connected drive, mount and copy as needed"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
